{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用numpy实现简单的两层神经网络\n",
    "\n",
    "梯度下降理解：https://blog.csdn.net/u014696921/article/details/61194741\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 torch 实现上述代码\n",
    "\n",
    "需要注意的几个点：\n",
    "`torch.mm(mat1, mat2, out=None)`: 实现矩阵乘法 \n",
    "`torch.clamp(input, min, max)`: 让张量中所有元素都保持在\\[min,max\\] 区间内\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)  \n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现自动梯度\n",
    "\n",
    "之前的代码是手动实现梯度的后向传播，这里使用Pytorch的自动梯度自动实现梯度传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Pytorch.nn 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 643.28369140625\n",
      "1 597.2950439453125\n",
      "2 556.8828735351562\n",
      "3 520.796142578125\n",
      "4 488.4062805175781\n",
      "5 459.2258605957031\n",
      "6 432.7748107910156\n",
      "7 408.33770751953125\n",
      "8 385.5289306640625\n",
      "9 364.169921875\n",
      "10 344.2653503417969\n",
      "11 325.7679138183594\n",
      "12 308.38525390625\n",
      "13 291.91009521484375\n",
      "14 276.2677917480469\n",
      "15 261.3163146972656\n",
      "16 247.05361938476562\n",
      "17 233.44827270507812\n",
      "18 220.52520751953125\n",
      "19 208.258056640625\n",
      "20 196.55555725097656\n",
      "21 185.43882751464844\n",
      "22 174.8434600830078\n",
      "23 164.78163146972656\n",
      "24 155.19577026367188\n",
      "25 146.1186981201172\n",
      "26 137.5124053955078\n",
      "27 129.33731079101562\n",
      "28 121.60320281982422\n",
      "29 114.26940155029297\n",
      "30 107.36672973632812\n",
      "31 100.87289428710938\n",
      "32 94.75439453125\n",
      "33 88.98796844482422\n",
      "34 83.559326171875\n",
      "35 78.45349884033203\n",
      "36 73.65199279785156\n",
      "37 69.14154815673828\n",
      "38 64.9059066772461\n",
      "39 60.92357635498047\n",
      "40 57.19109344482422\n",
      "41 53.689151763916016\n",
      "42 50.41056823730469\n",
      "43 47.34016418457031\n",
      "44 44.46343231201172\n",
      "45 41.76721954345703\n",
      "46 39.24153137207031\n",
      "47 36.880802154541016\n",
      "48 34.66419219970703\n",
      "49 32.590824127197266\n",
      "50 30.652406692504883\n",
      "51 28.8403377532959\n",
      "52 27.14507484436035\n",
      "53 25.558218002319336\n",
      "54 24.07244300842285\n",
      "55 22.679288864135742\n",
      "56 21.371868133544922\n",
      "57 20.145709991455078\n",
      "58 18.995237350463867\n",
      "59 17.916282653808594\n",
      "60 16.90534019470215\n",
      "61 15.958194732666016\n",
      "62 15.071015357971191\n",
      "63 14.237308502197266\n",
      "64 13.455385208129883\n",
      "65 12.721370697021484\n",
      "66 12.032154083251953\n",
      "67 11.384498596191406\n",
      "68 10.776638984680176\n",
      "69 10.204646110534668\n",
      "70 9.66594409942627\n",
      "71 9.159712791442871\n",
      "72 8.683323860168457\n",
      "73 8.235452651977539\n",
      "74 7.813965320587158\n",
      "75 7.417272567749023\n",
      "76 7.042935848236084\n",
      "77 6.689449310302734\n",
      "78 6.355541706085205\n",
      "79 6.04076623916626\n",
      "80 5.743473052978516\n",
      "81 5.462728023529053\n",
      "82 5.197503566741943\n",
      "83 4.9471354484558105\n",
      "84 4.710134506225586\n",
      "85 4.485592365264893\n",
      "86 4.2725510597229\n",
      "87 4.070366382598877\n",
      "88 3.878779649734497\n",
      "89 3.6973793506622314\n",
      "90 3.525270938873291\n",
      "91 3.362053632736206\n",
      "92 3.2073183059692383\n",
      "93 3.0603933334350586\n",
      "94 2.9208309650421143\n",
      "95 2.7882258892059326\n",
      "96 2.66200590133667\n",
      "97 2.5419504642486572\n",
      "98 2.4277184009552\n",
      "99 2.319187879562378\n",
      "100 2.2159347534179688\n",
      "101 2.1177594661712646\n",
      "102 2.0242676734924316\n",
      "103 1.9354621171951294\n",
      "104 1.8508985042572021\n",
      "105 1.7703347206115723\n",
      "106 1.6934971809387207\n",
      "107 1.6203100681304932\n",
      "108 1.550611972808838\n",
      "109 1.4840850830078125\n",
      "110 1.4206249713897705\n",
      "111 1.359911322593689\n",
      "112 1.3020521402359009\n",
      "113 1.2469472885131836\n",
      "114 1.1943271160125732\n",
      "115 1.1440927982330322\n",
      "116 1.0960822105407715\n",
      "117 1.0502650737762451\n",
      "118 1.0065059661865234\n",
      "119 0.9646721482276917\n",
      "120 0.9246445894241333\n",
      "121 0.8864290118217468\n",
      "122 0.8498852252960205\n",
      "123 0.8149157166481018\n",
      "124 0.7814972400665283\n",
      "125 0.7495062351226807\n",
      "126 0.7189039587974548\n",
      "127 0.6896401047706604\n",
      "128 0.6616542935371399\n",
      "129 0.6348426342010498\n",
      "130 0.609186053276062\n",
      "131 0.5846304893493652\n",
      "132 0.5611056685447693\n",
      "133 0.5385974645614624\n",
      "134 0.5170329213142395\n",
      "135 0.49639633297920227\n",
      "136 0.4766297936439514\n",
      "137 0.45767417550086975\n",
      "138 0.43950939178466797\n",
      "139 0.42206451296806335\n",
      "140 0.4053645133972168\n",
      "141 0.389344722032547\n",
      "142 0.37397000193595886\n",
      "143 0.3592587113380432\n",
      "144 0.34514498710632324\n",
      "145 0.3316153287887573\n",
      "146 0.3186468780040741\n",
      "147 0.3062038719654083\n",
      "148 0.29427263140678406\n",
      "149 0.28281310200691223\n",
      "150 0.27183833718299866\n",
      "151 0.2612980604171753\n",
      "152 0.2511872947216034\n",
      "153 0.24148915708065033\n",
      "154 0.23218122124671936\n",
      "155 0.22324666380882263\n",
      "156 0.2146766632795334\n",
      "157 0.2064502239227295\n",
      "158 0.19854849576950073\n",
      "159 0.19096669554710388\n",
      "160 0.18368415534496307\n",
      "161 0.17669279873371124\n",
      "162 0.1699693351984024\n",
      "163 0.16350746154785156\n",
      "164 0.15730158984661102\n",
      "165 0.15134792029857635\n",
      "166 0.1456279456615448\n",
      "167 0.14012950658798218\n",
      "168 0.1348477303981781\n",
      "169 0.1297736018896103\n",
      "170 0.12490023672580719\n",
      "171 0.1202174499630928\n",
      "172 0.11572331935167313\n",
      "173 0.11140096932649612\n",
      "174 0.1072470098733902\n",
      "175 0.10325755178928375\n",
      "176 0.0994187593460083\n",
      "177 0.09572987258434296\n",
      "178 0.09218299388885498\n",
      "179 0.08877360820770264\n",
      "180 0.08549529314041138\n",
      "181 0.0823441594839096\n",
      "182 0.07931246608495712\n",
      "183 0.07639831304550171\n",
      "184 0.07359744608402252\n",
      "185 0.07090329378843307\n",
      "186 0.06831060349941254\n",
      "187 0.06581630557775497\n",
      "188 0.06341791152954102\n",
      "189 0.06110909581184387\n",
      "190 0.05888960510492325\n",
      "191 0.05676152929663658\n",
      "192 0.054710254073143005\n",
      "193 0.0527438260614872\n",
      "194 0.05085284262895584\n",
      "195 0.04903325438499451\n",
      "196 0.0472823828458786\n",
      "197 0.04559733346104622\n",
      "198 0.043975479900836945\n",
      "199 0.04241383075714111\n",
      "200 0.04091028869152069\n",
      "201 0.039461974054574966\n",
      "202 0.03806617483496666\n",
      "203 0.03672303631901741\n",
      "204 0.03542906790971756\n",
      "205 0.034182772040367126\n",
      "206 0.032982006669044495\n",
      "207 0.03182510286569595\n",
      "208 0.0307106114923954\n",
      "209 0.0296371728181839\n",
      "210 0.028602220118045807\n",
      "211 0.02760501578450203\n",
      "212 0.02664402686059475\n",
      "213 0.025718197226524353\n",
      "214 0.02482495829463005\n",
      "215 0.02396460250020027\n",
      "216 0.02313537709414959\n",
      "217 0.02233610302209854\n",
      "218 0.021565435454249382\n",
      "219 0.02082216925919056\n",
      "220 0.020105881616473198\n",
      "221 0.019415689632296562\n",
      "222 0.018749674782156944\n",
      "223 0.018107742071151733\n",
      "224 0.017488718032836914\n",
      "225 0.016891563311219215\n",
      "226 0.016316181048750877\n",
      "227 0.015760891139507294\n",
      "228 0.01522485725581646\n",
      "229 0.014707845635712147\n",
      "230 0.014209010638296604\n",
      "231 0.013727647252380848\n",
      "232 0.013263395056128502\n",
      "233 0.012815432623028755\n",
      "234 0.012382983230054379\n",
      "235 0.011965913698077202\n",
      "236 0.01156332902610302\n",
      "237 0.011174625717103481\n",
      "238 0.010799706913530827\n",
      "239 0.010437740944325924\n",
      "240 0.01008821465075016\n",
      "241 0.009750817902386189\n",
      "242 0.00942535325884819\n",
      "243 0.009111222811043262\n",
      "244 0.008808128535747528\n",
      "245 0.008515282534062862\n",
      "246 0.00823266338557005\n",
      "247 0.007959611713886261\n",
      "248 0.007696195505559444\n",
      "249 0.007441545370966196\n",
      "250 0.007195663172751665\n",
      "251 0.00695815309882164\n",
      "252 0.0067289723083376884\n",
      "253 0.006507445592433214\n",
      "254 0.0062934719026088715\n",
      "255 0.006086780223995447\n",
      "256 0.005886999890208244\n",
      "257 0.005694014485925436\n",
      "258 0.005507561378180981\n",
      "259 0.005327405873686075\n",
      "260 0.005153463687747717\n",
      "261 0.0049854121170938015\n",
      "262 0.004822961054742336\n",
      "263 0.004665931221097708\n",
      "264 0.004514157772064209\n",
      "265 0.004367489833384752\n",
      "266 0.004225697834044695\n",
      "267 0.0040888539515435696\n",
      "268 0.003956417553126812\n",
      "269 0.003828365821391344\n",
      "270 0.0037046114448457956\n",
      "271 0.003585011465474963\n",
      "272 0.003469385439530015\n",
      "273 0.0033575668931007385\n",
      "274 0.0032494498882442713\n",
      "275 0.0031449547968804836\n",
      "276 0.003043900243937969\n",
      "277 0.002946270862594247\n",
      "278 0.0028518042527139187\n",
      "279 0.0027604526840150356\n",
      "280 0.0026720815803855658\n",
      "281 0.002586677670478821\n",
      "282 0.002504047006368637\n",
      "283 0.0024241418577730656\n",
      "284 0.002346847439184785\n",
      "285 0.0022721057757735252\n",
      "286 0.0021997939329594374\n",
      "287 0.002129883971065283\n",
      "288 0.002062209416180849\n",
      "289 0.0019967963453382254\n",
      "290 0.0019335265969857574\n",
      "291 0.001872287830337882\n",
      "292 0.001812998321838677\n",
      "293 0.0017556684324517846\n",
      "294 0.0017001846572384238\n",
      "295 0.001646529883146286\n",
      "296 0.0015945881605148315\n",
      "297 0.0015443372540175915\n",
      "298 0.0014957217499613762\n",
      "299 0.0014486752916127443\n",
      "300 0.0014031324535608292\n",
      "301 0.0013590676244348288\n",
      "302 0.0013164213160052896\n",
      "303 0.0012751756003126502\n",
      "304 0.0012351983459666371\n",
      "305 0.001196540892124176\n",
      "306 0.001159080769866705\n",
      "307 0.001122867688536644\n",
      "308 0.0010877976892516017\n",
      "309 0.001053827116265893\n",
      "310 0.0010209628380835056\n",
      "311 0.000989166204817593\n",
      "312 0.0009583646897226572\n",
      "313 0.0009285383857786655\n",
      "314 0.0008996729156933725\n",
      "315 0.0008717247401364148\n",
      "316 0.0008446769788861275\n",
      "317 0.0008184813195839524\n",
      "318 0.0007931047584861517\n",
      "319 0.0007685628370381892\n",
      "320 0.0007447723764926195\n",
      "321 0.0007217337843030691\n",
      "322 0.0006994356517679989\n",
      "323 0.0006778307142667472\n",
      "324 0.0006569083197973669\n",
      "325 0.0006366494926624\n",
      "326 0.0006170383421704173\n",
      "327 0.0005980310961604118\n",
      "328 0.0005796320619992912\n",
      "329 0.0005617960123345256\n",
      "330 0.0005445333663374186\n",
      "331 0.000527786323800683\n",
      "332 0.0005115931853652\n",
      "333 0.0004959024954587221\n",
      "334 0.0004806993529200554\n",
      "335 0.0004659704864025116\n",
      "336 0.0004517041379585862\n",
      "337 0.00043789445771835744\n",
      "338 0.0004244875453878194\n",
      "339 0.0004115241172257811\n",
      "340 0.000398952019168064\n",
      "341 0.0003867766645271331\n",
      "342 0.00037498248275369406\n",
      "343 0.0003635440662037581\n",
      "344 0.00035246601328253746\n",
      "345 0.00034173447056673467\n",
      "346 0.0003313292399980128\n",
      "347 0.00032124973949976265\n",
      "348 0.000311478681396693\n",
      "349 0.0003020194126293063\n",
      "350 0.00029285106575116515\n",
      "351 0.00028395879780873656\n",
      "352 0.00027534677064977586\n",
      "353 0.00026700025773607194\n",
      "354 0.0002589093055576086\n",
      "355 0.00025106690009124577\n",
      "356 0.0002434680500300601\n",
      "357 0.00023610521748196334\n",
      "358 0.0002289720141561702\n",
      "359 0.00022205442655831575\n",
      "360 0.00021534584811888635\n",
      "361 0.00020884867990389466\n",
      "362 0.0002025461581069976\n",
      "363 0.00019644519488792866\n",
      "364 0.00019051905837841332\n",
      "365 0.000184780583367683\n",
      "366 0.0001792172115528956\n",
      "367 0.00017382123041898012\n",
      "368 0.00016859268362168223\n",
      "369 0.00016352128295693547\n",
      "370 0.00015860892017371953\n",
      "371 0.0001538380020065233\n",
      "372 0.00014922597620170563\n",
      "373 0.0001447488320991397\n",
      "374 0.00014040821406524628\n",
      "375 0.00013619253877550364\n",
      "376 0.00013211554323788732\n",
      "377 0.00012815662194043398\n",
      "378 0.00012431788491085172\n",
      "379 0.00012060383596690372\n",
      "380 0.00011699544120347127\n",
      "381 0.00011349625128787011\n",
      "382 0.0001101007655961439\n",
      "383 0.00010681370622478426\n",
      "384 0.00010362317698309198\n",
      "385 0.00010052825382445008\n",
      "386 9.753141057444736e-05\n",
      "387 9.46241634665057e-05\n",
      "388 9.180200140690431e-05\n",
      "389 8.906494622351602e-05\n",
      "390 8.641292515676469e-05\n",
      "391 8.383943350054324e-05\n",
      "392 8.134634845191613e-05\n",
      "393 7.892517896834761e-05\n",
      "394 7.657728565391153e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 7.430157711496577e-05\n",
      "396 7.209385512396693e-05\n",
      "397 6.995402509346604e-05\n",
      "398 6.787633174099028e-05\n",
      "399 6.586137897102162e-05\n",
      "400 6.390771886799484e-05\n",
      "401 6.201482028700411e-05\n",
      "402 6.017503619659692e-05\n",
      "403 5.83930013817735e-05\n",
      "404 5.666098149958998e-05\n",
      "405 5.4980813729343936e-05\n",
      "406 5.335424066288397e-05\n",
      "407 5.177417551749386e-05\n",
      "408 5.024269921705127e-05\n",
      "409 4.8757960030343384e-05\n",
      "410 4.731717854156159e-05\n",
      "411 4.59186194348149e-05\n",
      "412 4.456163151189685e-05\n",
      "413 4.324650944909081e-05\n",
      "414 4.197130328975618e-05\n",
      "415 4.0732964407652617e-05\n",
      "416 3.95305069105234e-05\n",
      "417 3.836559335468337e-05\n",
      "418 3.7234021874610335e-05\n",
      "419 3.613745138864033e-05\n",
      "420 3.5071687307208776e-05\n",
      "421 3.40399237757083e-05\n",
      "422 3.303894118289463e-05\n",
      "423 3.2064846891444176e-05\n",
      "424 3.1123345252126455e-05\n",
      "425 3.020418807864189e-05\n",
      "426 2.931612471002154e-05\n",
      "427 2.8455919164116494e-05\n",
      "428 2.762027725111693e-05\n",
      "429 2.6805480956682004e-05\n",
      "430 2.601906999188941e-05\n",
      "431 2.5256416847696528e-05\n",
      "432 2.451376167300623e-05\n",
      "433 2.3795209926902317e-05\n",
      "434 2.3095957658370025e-05\n",
      "435 2.2419359083869494e-05\n",
      "436 2.1760124582215212e-05\n",
      "437 2.1123663827893324e-05\n",
      "438 2.0504141502897255e-05\n",
      "439 1.99021196749527e-05\n",
      "440 1.9320554201840423e-05\n",
      "441 1.8755261407932267e-05\n",
      "442 1.820466604840476e-05\n",
      "443 1.7672249668976292e-05\n",
      "444 1.7156588000943884e-05\n",
      "445 1.665273157414049e-05\n",
      "446 1.61660118465079e-05\n",
      "447 1.569402957102284e-05\n",
      "448 1.5234633792715613e-05\n",
      "449 1.4789078704779968e-05\n",
      "450 1.4358272892422974e-05\n",
      "451 1.3938237316324376e-05\n",
      "452 1.3531622244045138e-05\n",
      "453 1.3136668712832034e-05\n",
      "454 1.2752345355693251e-05\n",
      "455 1.2380577572912443e-05\n",
      "456 1.2019537280139048e-05\n",
      "457 1.1668116712826304e-05\n",
      "458 1.1328522305120714e-05\n",
      "459 1.0999174264725298e-05\n",
      "460 1.0677406862669159e-05\n",
      "461 1.0367390132159926e-05\n",
      "462 1.006561524263816e-05\n",
      "463 9.771186341822613e-06\n",
      "464 9.486888302490115e-06\n",
      "465 9.21163609746145e-06\n",
      "466 8.943226021074224e-06\n",
      "467 8.68275310494937e-06\n",
      "468 8.430504749412648e-06\n",
      "469 8.185365004464984e-06\n",
      "470 7.946427103888709e-06\n",
      "471 7.71565919421846e-06\n",
      "472 7.491238648071885e-06\n",
      "473 7.272818493220257e-06\n",
      "474 7.0623796091240365e-06\n",
      "475 6.856204436189728e-06\n",
      "476 6.658468919340521e-06\n",
      "477 6.464549187512603e-06\n",
      "478 6.276451131270733e-06\n",
      "479 6.094378932175459e-06\n",
      "480 5.918197530263569e-06\n",
      "481 5.746622264268808e-06\n",
      "482 5.579175649472745e-06\n",
      "483 5.417103693616809e-06\n",
      "484 5.260700163489673e-06\n",
      "485 5.107880951982224e-06\n",
      "486 4.959521447744919e-06\n",
      "487 4.816151886188891e-06\n",
      "488 4.675839136325521e-06\n",
      "489 4.540872851066524e-06\n",
      "490 4.408893346408149e-06\n",
      "491 4.281405836081831e-06\n",
      "492 4.157921466685366e-06\n",
      "493 4.037980488647008e-06\n",
      "494 3.919926257367479e-06\n",
      "495 3.806772156167426e-06\n",
      "496 3.6966173411201453e-06\n",
      "497 3.5896939607482636e-06\n",
      "498 3.4858112485380843e-06\n",
      "499 3.384728415767313e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用其他优化器优化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 695.9238891601562\n",
      "1 678.3800659179688\n",
      "2 661.4261474609375\n",
      "3 644.9351196289062\n",
      "4 628.90478515625\n",
      "5 613.3997192382812\n",
      "6 598.31005859375\n",
      "7 583.52880859375\n",
      "8 569.1648559570312\n",
      "9 555.2843627929688\n",
      "10 541.8772583007812\n",
      "11 528.8619384765625\n",
      "12 516.2294921875\n",
      "13 503.99652099609375\n",
      "14 492.0730895996094\n",
      "15 480.4508361816406\n",
      "16 469.14093017578125\n",
      "17 458.1546936035156\n",
      "18 447.4594421386719\n",
      "19 437.10137939453125\n",
      "20 427.000732421875\n",
      "21 417.1375732421875\n",
      "22 407.4768981933594\n",
      "23 398.0486755371094\n",
      "24 388.8899841308594\n",
      "25 379.98077392578125\n",
      "26 371.2334899902344\n",
      "27 362.6468200683594\n",
      "28 354.21588134765625\n",
      "29 345.9257507324219\n",
      "30 337.8188781738281\n",
      "31 329.9184265136719\n",
      "32 322.2074890136719\n",
      "33 314.6590881347656\n",
      "34 307.2610778808594\n",
      "35 300.01043701171875\n",
      "36 292.91754150390625\n",
      "37 285.9495849609375\n",
      "38 279.0962219238281\n",
      "39 272.39044189453125\n",
      "40 265.8233337402344\n",
      "41 259.3749084472656\n",
      "42 253.0572967529297\n",
      "43 246.89080810546875\n",
      "44 240.84866333007812\n",
      "45 234.94166564941406\n",
      "46 229.15016174316406\n",
      "47 223.4879608154297\n",
      "48 217.9470672607422\n",
      "49 212.52796936035156\n",
      "50 207.21104431152344\n",
      "51 202.0169219970703\n",
      "52 196.93150329589844\n",
      "53 191.94229125976562\n",
      "54 187.0681610107422\n",
      "55 182.28970336914062\n",
      "56 177.62509155273438\n",
      "57 173.0522003173828\n",
      "58 168.57125854492188\n",
      "59 164.18453979492188\n",
      "60 159.89202880859375\n",
      "61 155.6963653564453\n",
      "62 151.58741760253906\n",
      "63 147.5714874267578\n",
      "64 143.65463256835938\n",
      "65 139.82383728027344\n",
      "66 136.06893920898438\n",
      "67 132.40435791015625\n",
      "68 128.81642150878906\n",
      "69 125.30152893066406\n",
      "70 121.86933135986328\n",
      "71 118.51463317871094\n",
      "72 115.2423324584961\n",
      "73 112.04094696044922\n",
      "74 108.9162368774414\n",
      "75 105.8677978515625\n",
      "76 102.89073944091797\n",
      "77 99.9811019897461\n",
      "78 97.13546752929688\n",
      "79 94.35713195800781\n",
      "80 91.64358520507812\n",
      "81 88.99613952636719\n",
      "82 86.41557312011719\n",
      "83 83.89993286132812\n",
      "84 81.4382553100586\n",
      "85 79.03460693359375\n",
      "86 76.69189453125\n",
      "87 74.41258239746094\n",
      "88 72.18840026855469\n",
      "89 70.01483154296875\n",
      "90 67.89141082763672\n",
      "91 65.82404327392578\n",
      "92 63.80815124511719\n",
      "93 61.8421745300293\n",
      "94 59.92558288574219\n",
      "95 58.061988830566406\n",
      "96 56.24830627441406\n",
      "97 54.47837448120117\n",
      "98 52.75614547729492\n",
      "99 51.078102111816406\n",
      "100 49.444541931152344\n",
      "101 47.854347229003906\n",
      "102 46.309329986572266\n",
      "103 44.8046875\n",
      "104 43.33952331542969\n",
      "105 41.91242218017578\n",
      "106 40.52366638183594\n",
      "107 39.173404693603516\n",
      "108 37.859588623046875\n",
      "109 36.58356475830078\n",
      "110 35.34247589111328\n",
      "111 34.13654708862305\n",
      "112 32.96468734741211\n",
      "113 31.82493782043457\n",
      "114 30.717384338378906\n",
      "115 29.6436710357666\n",
      "116 28.599822998046875\n",
      "117 27.588998794555664\n",
      "118 26.608306884765625\n",
      "119 25.655319213867188\n",
      "120 24.732057571411133\n",
      "121 23.837129592895508\n",
      "122 22.968111038208008\n",
      "123 22.125600814819336\n",
      "124 21.309528350830078\n",
      "125 20.518688201904297\n",
      "126 19.752634048461914\n",
      "127 19.012563705444336\n",
      "128 18.29584312438965\n",
      "129 17.602779388427734\n",
      "130 16.932527542114258\n",
      "131 16.283615112304688\n",
      "132 15.655754089355469\n",
      "133 15.048078536987305\n",
      "134 14.460672378540039\n",
      "135 13.892291069030762\n",
      "136 13.342625617980957\n",
      "137 12.812448501586914\n",
      "138 12.299040794372559\n",
      "139 11.803803443908691\n",
      "140 11.325401306152344\n",
      "141 10.863040924072266\n",
      "142 10.417217254638672\n",
      "143 9.98684024810791\n",
      "144 9.571435928344727\n",
      "145 9.171584129333496\n",
      "146 8.78576946258545\n",
      "147 8.414134979248047\n",
      "148 8.056245803833008\n",
      "149 7.711586952209473\n",
      "150 7.380046367645264\n",
      "151 7.060811519622803\n",
      "152 6.753942489624023\n",
      "153 6.458981513977051\n",
      "154 6.175381660461426\n",
      "155 5.902821063995361\n",
      "156 5.640903949737549\n",
      "157 5.389509201049805\n",
      "158 5.14808464050293\n",
      "159 4.916115760803223\n",
      "160 4.693770408630371\n",
      "161 4.480289459228516\n",
      "162 4.275753974914551\n",
      "163 4.079862117767334\n",
      "164 3.8922858238220215\n",
      "165 3.712613582611084\n",
      "166 3.5404796600341797\n",
      "167 3.3757596015930176\n",
      "168 3.2181174755096436\n",
      "169 3.067209482192993\n",
      "170 2.9229159355163574\n",
      "171 2.785322427749634\n",
      "172 2.65397572517395\n",
      "173 2.528383493423462\n",
      "174 2.40844464302063\n",
      "175 2.293975353240967\n",
      "176 2.1845028400421143\n",
      "177 2.080094814300537\n",
      "178 1.9803351163864136\n",
      "179 1.8852351903915405\n",
      "180 1.7945122718811035\n",
      "181 1.7080795764923096\n",
      "182 1.6256259679794312\n",
      "183 1.5468980073928833\n",
      "184 1.4720197916030884\n",
      "185 1.4005693197250366\n",
      "186 1.3325393199920654\n",
      "187 1.2677395343780518\n",
      "188 1.205926775932312\n",
      "189 1.147214651107788\n",
      "190 1.0912669897079468\n",
      "191 1.0380409955978394\n",
      "192 0.9874147772789001\n",
      "193 0.9392070174217224\n",
      "194 0.8933417201042175\n",
      "195 0.8496850728988647\n",
      "196 0.8081738948822021\n",
      "197 0.7687188386917114\n",
      "198 0.7312268018722534\n",
      "199 0.6955389380455017\n",
      "200 0.6616118550300598\n",
      "201 0.6293585896492004\n",
      "202 0.598662257194519\n",
      "203 0.5695431232452393\n",
      "204 0.541862964630127\n",
      "205 0.5155274271965027\n",
      "206 0.49049150943756104\n",
      "207 0.46670717000961304\n",
      "208 0.44413506984710693\n",
      "209 0.4226560890674591\n",
      "210 0.40220335125923157\n",
      "211 0.38275381922721863\n",
      "212 0.3642827868461609\n",
      "213 0.34667834639549255\n",
      "214 0.32994115352630615\n",
      "215 0.3140215575695038\n",
      "216 0.2988895773887634\n",
      "217 0.28448018431663513\n",
      "218 0.2707942724227905\n",
      "219 0.25776609778404236\n",
      "220 0.24537056684494019\n",
      "221 0.23358255624771118\n",
      "222 0.22237160801887512\n",
      "223 0.21171808242797852\n",
      "224 0.20157775282859802\n",
      "225 0.19193071126937866\n",
      "226 0.18278105556964874\n",
      "227 0.17407509684562683\n",
      "228 0.16579610109329224\n",
      "229 0.1579202264547348\n",
      "230 0.1504243016242981\n",
      "231 0.1433010697364807\n",
      "232 0.13652415573596954\n",
      "233 0.13007646799087524\n",
      "234 0.12393887341022491\n",
      "235 0.11810531467199326\n",
      "236 0.11256477981805801\n",
      "237 0.10728707164525986\n",
      "238 0.10226843506097794\n",
      "239 0.0974961444735527\n",
      "240 0.09295863658189774\n",
      "241 0.08864405006170273\n",
      "242 0.08453787863254547\n",
      "243 0.08062685281038284\n",
      "244 0.07690674066543579\n",
      "245 0.07336579263210297\n",
      "246 0.06998814642429352\n",
      "247 0.06678587943315506\n",
      "248 0.06374070048332214\n",
      "249 0.06083919480443001\n",
      "250 0.05807848647236824\n",
      "251 0.05544885993003845\n",
      "252 0.05294211953878403\n",
      "253 0.05055602639913559\n",
      "254 0.04828556254506111\n",
      "255 0.04611198231577873\n",
      "256 0.04404674097895622\n",
      "257 0.042079515755176544\n",
      "258 0.04020322486758232\n",
      "259 0.03841433301568031\n",
      "260 0.03670801967382431\n",
      "261 0.03507978096604347\n",
      "262 0.03352614492177963\n",
      "263 0.032043617218732834\n",
      "264 0.030628858134150505\n",
      "265 0.029279429465532303\n",
      "266 0.027990663424134254\n",
      "267 0.026761025190353394\n",
      "268 0.025587281212210655\n",
      "269 0.024466989561915398\n",
      "270 0.023397298529744148\n",
      "271 0.022376127541065216\n",
      "272 0.02140146680176258\n",
      "273 0.020470593124628067\n",
      "274 0.01958162523806095\n",
      "275 0.018733082339167595\n",
      "276 0.017922334372997284\n",
      "277 0.01714780181646347\n",
      "278 0.016408441588282585\n",
      "279 0.015702230855822563\n",
      "280 0.01502724178135395\n",
      "281 0.014383229427039623\n",
      "282 0.013766764663159847\n",
      "283 0.013177969492971897\n",
      "284 0.012615431100130081\n",
      "285 0.012077723629772663\n",
      "286 0.011563967913389206\n",
      "287 0.011072378605604172\n",
      "288 0.010602611117064953\n",
      "289 0.010153450071811676\n",
      "290 0.009724041447043419\n",
      "291 0.00931334774941206\n",
      "292 0.008920475840568542\n",
      "293 0.008544876240193844\n",
      "294 0.008185436949133873\n",
      "295 0.007841759361326694\n",
      "296 0.007513007614761591\n",
      "297 0.0071984343230724335\n",
      "298 0.006897370796650648\n",
      "299 0.006609306205064058\n",
      "300 0.006333629135042429\n",
      "301 0.006069865543395281\n",
      "302 0.0058174231089651585\n",
      "303 0.005575800780206919\n",
      "304 0.00534448679536581\n",
      "305 0.005123026669025421\n",
      "306 0.004911015275865793\n",
      "307 0.004708120133727789\n",
      "308 0.004513961728662252\n",
      "309 0.004327917471528053\n",
      "310 0.0041498057544231415\n",
      "311 0.0039792838506400585\n",
      "312 0.0038162777200341225\n",
      "313 0.00366009958088398\n",
      "314 0.0035105617716908455\n",
      "315 0.003367305500432849\n",
      "316 0.0032300460152328014\n",
      "317 0.0030985712073743343\n",
      "318 0.0029725388158112764\n",
      "319 0.002851750934496522\n",
      "320 0.002736044116318226\n",
      "321 0.002625146182253957\n",
      "322 0.002518877387046814\n",
      "323 0.002417013281956315\n",
      "324 0.0023193499073386192\n",
      "325 0.002225740347057581\n",
      "326 0.002136013936251402\n",
      "327 0.0020500419195741415\n",
      "328 0.0019675027579069138\n",
      "329 0.0018884249730035663\n",
      "330 0.0018126005306839943\n",
      "331 0.0017398621421307325\n",
      "332 0.0016701348358765244\n",
      "333 0.001603257842361927\n",
      "334 0.0015391381457448006\n",
      "335 0.0014776191674172878\n",
      "336 0.001418598461896181\n",
      "337 0.0013620087411254644\n",
      "338 0.0013077270705252886\n",
      "339 0.0012556544970721006\n",
      "340 0.0012056977720931172\n",
      "341 0.001157762948423624\n",
      "342 0.001111792866140604\n",
      "343 0.0010676780948415399\n",
      "344 0.0010253608925268054\n",
      "345 0.0009847318287938833\n",
      "346 0.0009457736741751432\n",
      "347 0.0009083673940040171\n",
      "348 0.0008724828949198127\n",
      "349 0.0008380301878787577\n",
      "350 0.0008049843600019813\n",
      "351 0.0007732695667073131\n",
      "352 0.0007428245153278112\n",
      "353 0.0007135929190553725\n",
      "354 0.0006855496903881431\n",
      "355 0.0006586225936189294\n",
      "356 0.0006327723385766149\n",
      "357 0.0006079584127292037\n",
      "358 0.0005841318634338677\n",
      "359 0.0005612669629044831\n",
      "360 0.0005393187166191638\n",
      "361 0.0005182272871024907\n",
      "362 0.0004979904042556882\n",
      "363 0.0004785521887242794\n",
      "364 0.00045989261707291007\n",
      "365 0.0004419779288582504\n",
      "366 0.00042476356611587107\n",
      "367 0.0004082405648659915\n",
      "368 0.00039237216697074473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 0.0003771348565351218\n",
      "370 0.00036249152617529035\n",
      "371 0.00034843155299313366\n",
      "372 0.00033492734655737877\n",
      "373 0.00032195402309298515\n",
      "374 0.00030949435313232243\n",
      "375 0.00029752214322797954\n",
      "376 0.0002860495005734265\n",
      "377 0.00027498567942529917\n",
      "378 0.0002643777406774461\n",
      "379 0.00025417658616788685\n",
      "380 0.0002443832054268569\n",
      "381 0.0002349685673834756\n",
      "382 0.00022592837922275066\n",
      "383 0.00021723764075431973\n",
      "384 0.0002088883047690615\n",
      "385 0.00020086603763047606\n",
      "386 0.00019315406098030508\n",
      "387 0.0001857435709098354\n",
      "388 0.00017862294043879956\n",
      "389 0.0001717728446237743\n",
      "390 0.00016519696509931237\n",
      "391 0.00015887411427684128\n",
      "392 0.00015279340732377023\n",
      "393 0.00014695426216349006\n",
      "394 0.00014133923104964197\n",
      "395 0.00013594118354376405\n",
      "396 0.00013075326569378376\n",
      "397 0.00012576814333442599\n",
      "398 0.00012097253056708723\n",
      "399 0.00011635934060905129\n",
      "400 0.0001119296794058755\n",
      "401 0.0001076648331945762\n",
      "402 0.00010357092105550691\n",
      "403 9.963364573195577e-05\n",
      "404 9.584342478774488e-05\n",
      "405 9.220342326443642e-05\n",
      "406 8.870114106684923e-05\n",
      "407 8.533438813174143e-05\n",
      "408 8.209335646824911e-05\n",
      "409 7.898238982306793e-05\n",
      "410 7.59874819777906e-05\n",
      "411 7.310498767765239e-05\n",
      "412 7.033740257611498e-05\n",
      "413 6.76751442370005e-05\n",
      "414 6.51121445116587e-05\n",
      "415 6.264863623073325e-05\n",
      "416 6.027601921232417e-05\n",
      "417 5.799806604045443e-05\n",
      "418 5.5804859584895894e-05\n",
      "419 5.369430800783448e-05\n",
      "420 5.166678602108732e-05\n",
      "421 4.971600355929695e-05\n",
      "422 4.7836630983510986e-05\n",
      "423 4.603110210155137e-05\n",
      "424 4.4291929953033105e-05\n",
      "425 4.262054790160619e-05\n",
      "426 4.101323793292977e-05\n",
      "427 3.946565630030818e-05\n",
      "428 3.797958925133571e-05\n",
      "429 3.654679312603548e-05\n",
      "430 3.516799915814772e-05\n",
      "431 3.384281080798246e-05\n",
      "432 3.256711352150887e-05\n",
      "433 3.133873542537913e-05\n",
      "434 3.015756738022901e-05\n",
      "435 2.9022115995758213e-05\n",
      "436 2.7930120268138126e-05\n",
      "437 2.6875357434619218e-05\n",
      "438 2.5862922484520823e-05\n",
      "439 2.4889295673347078e-05\n",
      "440 2.395133560639806e-05\n",
      "441 2.3048094590194523e-05\n",
      "442 2.218103145423811e-05\n",
      "443 2.134497117367573e-05\n",
      "444 2.0540574041660875e-05\n",
      "445 1.9767066987697035e-05\n",
      "446 1.9022345441044308e-05\n",
      "447 1.830436485761311e-05\n",
      "448 1.7615237084100954e-05\n",
      "449 1.6951309589785524e-05\n",
      "450 1.631195300433319e-05\n",
      "451 1.569624146213755e-05\n",
      "452 1.5104573321877979e-05\n",
      "453 1.453504228265956e-05\n",
      "454 1.3986566955281887e-05\n",
      "455 1.3457959539664444e-05\n",
      "456 1.294996945944149e-05\n",
      "457 1.2461665392038412e-05\n",
      "458 1.1990994607913308e-05\n",
      "459 1.15377633846947e-05\n",
      "460 1.1101309610239696e-05\n",
      "461 1.0681836101866793e-05\n",
      "462 1.0278191439283546e-05\n",
      "463 9.889722605294082e-06\n",
      "464 9.515661076875404e-06\n",
      "465 9.155161023954861e-06\n",
      "466 8.808700840745587e-06\n",
      "467 8.475293725496158e-06\n",
      "468 8.15381918073399e-06\n",
      "469 7.844299034331925e-06\n",
      "470 7.546309461758938e-06\n",
      "471 7.2606467256264295e-06\n",
      "472 6.984657829889329e-06\n",
      "473 6.720009423588635e-06\n",
      "474 6.464365924330195e-06\n",
      "475 6.218247108336072e-06\n",
      "476 5.981979938951554e-06\n",
      "477 5.753847744927043e-06\n",
      "478 5.535212949325796e-06\n",
      "479 5.323943696566857e-06\n",
      "480 5.120909008837771e-06\n",
      "481 4.925253506371519e-06\n",
      "482 4.737401468446478e-06\n",
      "483 4.556882231554482e-06\n",
      "484 4.382289262139238e-06\n",
      "485 4.214897217025282e-06\n",
      "486 4.053385964652989e-06\n",
      "487 3.897830083587905e-06\n",
      "488 3.748492872546194e-06\n",
      "489 3.6048784295417136e-06\n",
      "490 3.4664428767428035e-06\n",
      "491 3.3331778013234725e-06\n",
      "492 3.2050147638074122e-06\n",
      "493 3.0819023777439725e-06\n",
      "494 2.963263341371203e-06\n",
      "495 2.8489710075518815e-06\n",
      "496 2.7392463834985392e-06\n",
      "497 2.6336019800510257e-06\n",
      "498 2.5317208383057732e-06\n",
      "499 2.4337900867976714e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重点：定制自己的网络模型\n",
    "\n",
    "- 继承`nn.Module`类\n",
    "\n",
    "- 重构函数`forward` 以获得输入张量和得到输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
